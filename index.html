<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Foundation models must reason jointly over multiple modalities, but how well do they handle cross-modal contexts? We reveal that FMs recognize conflicts in unimodal contexts 90% of the time, but only 3% when evidence is split across modalities. We trace this failure to cross-modal attention imbalance and show that instance-level modality mixing‚Äîexplicitly combining multiple modalities within each training instance‚Äîsignificantly improves cross-modal reasoning without requiring additional data curation.">
  <meta property="og:title" content="Mitigating Modal Imbalance in Multimodal Reasoning"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://ar-forum.github.io/modal-imbalance/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/motivating_example.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Mitigating Modal Imbalance in Multimodal Reasoning">
  <meta name="twitter:description" content="We uncover cross-modal attention imbalance in foundation models and propose instance-level modality mixing to improve cross-modal reasoning.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/motivating_example.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Learning, Cross-Modal Reasoning, Attention Imbalance, Foundation Models, Vision-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mitigating Modal Imbalance in Multimodal Reasoning</title>
  <link rel="icon" type="image/x-icon" href="static/images/scale.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css?v=2">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mitigating Modal Imbalance in Multimodal Reasoning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://chenwu.io" target="_blank">Chen Henry Wu</a>*, 
                <span class="author-block">
                  <a href="https://neilkale.github.io" target="_blank">Neil Kale</a>*, 
                  <span class="author-block">
                    <a href="https://www.cs.cmu.edu/~aditirag/" target="_blank">Aditi Raghunathan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Carnegie Mellon University<br>COLM 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/pdf?id=JsaXxGOXfU" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AR-FORUM/modal-imbalance" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <img src="static/images/motivating_example.png" alt="Teaser Image" style="width: 100%; max-width: 1000px; height: auto; display: block; margin: 10px auto;">
      </center>
      <h2 class="subtitle has-text-centered">
      Foundation models deployed in real-world tasks must integrate diverse modalities‚Äîfrom multilingual news to shopping websites and medical records. However, we find that FMs recognize conflicts in <strong>unimodal contexts</strong> 90% of the time, but this drops to as low as <strong>3%</strong> when evidence is split across modalities. We trace this failure to <strong>cross-modal attention imbalance</strong> and show that <strong>instance-level modality mixing</strong> significantly improves cross-modal reasoning.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <ul style="font-size: 1.2em; line-height: 1.6;">
            <li><strong>üö´ The Problem:</strong> Foundation models fail at cross-modal reasoning - detecting conflicts in unimodal contexts (text-text, image-image) 90% of the time but only 3% in cross-modal contexts (text-image). This extends to multilingual settings too.</li>
            <li><strong>üîç The Cause:</strong> We identify <strong>cross-modal attention imbalance</strong> - FMs disproportionately prioritize certain modalities. Manual attention reweighting confirms this has a causal effect on reasoning performance.</li>
            <li><strong>‚úÖ The Solution:</strong> <strong>Instance-level modality mixing</strong> - explicitly combining multiple modalities within each training instance. This simple, scalable approach reduces attention imbalance by 4√ó and improves conflict detection by up to 37% without requiring additional data curation.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Paper poster -->
<section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Cross-Modal Reasoning: A Critical Gap</h2>
      <div class="content has-text-justified">
        <p>
          <strong>How good are foundation models at joint reasoning?</strong> Real-world tasks require FMs to simultaneously reason over multiple modalities. To test this, we create <strong>cross-modal conflict datasets</strong> where each modality provides contrasting evidence. This allows us to examine whether FMs prioritize one modality over another or reason jointly to reconcile the conflict.
        </p>
        <ul style="margin-left: 20px;">
          <li><strong>Cross-Modal QA (CMQA):</strong> Text vs. Image conflicts based on VQA-v2 dataset</li>
          <li><strong>Cross-Lingual QA (CLQA):</strong> English vs. Chinese (and other languages) conflicts</li>
        </ul>
        <p>
          <strong>Key Finding:</strong> FMs detect conflicts in unimodal contexts (Text-Text, Image-Image) far better than cross-modal contexts (Text-Image). The same pattern holds for multilingual settings.
        </p>

          <center>
          <img src="static/images/hetero_multilingual.png" alt="Multilingual Results" class="teaser-image" width="600px">
          <img src="static/images/hetero_multimodal.png" alt="Multimodal Results" class="teaser-image" width="230px">
          </center>
          <p>
            <center>
              <b><i>Fig 2: Cross-Modal Reasoning Failure.</i></b> FMs are significantly worse at reasoning over cross-modal contexts than unimodal contexts. Left: Multilingual setting shows English-English and Chinese-Chinese perform 5√ó better than English-Chinese. Right: Multimodal setting shows Text-Text and Image-Image far outperform Text-Image.
            </center>
          </p>

          <div style="text-align: center; margin: 30px 0; padding: 20px; background-color: #f5f5f5; border-left: 5px solid #3273dc;">
            <p style="font-size: 24px; font-weight: bold; color: #3273dc; margin: 0;">
              üö® State-of-the-art FMs fail at simple cross-modal reasoning tasks
            </p>
          </div>
          <div style="background-color: rgba(225, 200, 245, 0.4);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
            <p style="font-size: 20px; font-weight: bold;">üí° Key Insight</p>
            <p>This performance drop is NOT due to weakness in one modality‚ÄîVLMs detect Image-Image conflicts as easily as Text-Text conflicts. The problem arises specifically when reasoning across modalities within the same context.</p>
          </div>
      </div>
    </div>
  </section>
<!--End paper poster -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Cross-Modal Attention Imbalance: The Root Cause</h2>
      <div class="content has-text-justified">
        <p>
          <strong>Why do FMs fail at cross-modal reasoning?</strong> We hypothesize that context contribution in task-relevant subspaces is imbalanced in cross-modal contexts, making FMs more likely to rely on the dominant modality instead of doing conflict detection.
        </p>
          <h3 class="title">Visualizing Attention Imbalance</h3>

          <center>
          <img src="static/images/mental-1.png" alt="Attention Imbalance Visualization" class="imp-image" style="width: 800px !important; max-width: none !important; height: auto !important; display: block !important; margin: 10px auto !important;">
        </center>
          <p>
            We compute the average norm of context contributions (u_k) for each modality, averaged over all layers and attention heads. The figure shows that:</p>
            <ul style="margin-left: 20px;">
              <li><strong>Cross-lingual:</strong> English context contributes more than Chinese context</li>
              <li><strong>Cross-modal:</strong> Text contributes more than images</li>
            </ul>
   
        <div style="background-color: rgba(225, 200, 245, 0.4);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
          <p style="font-size: 20px; font-weight: bold;">üß† Mental Model</p>
          <p>In unimodal contexts, different domains show balanced normalized attention despite divergent pre-softmax logits. Cross-modal contexts expose attention imbalance‚Äînormalization fails to mitigate logit-level imbalance.</p>
        </div>
        <div style="margin-top: 30px;"></div>
      
        <h3 class="title">üî¨ Causal Intervention: Attention Manipulation</h3>
          <p>To test if there's a <strong>causal</strong> relationship between attention imbalance and cross-modal reasoning, we manipulate attention weights by adding a small constant Œµ to the unnormalized attention score of the underrepresented modality.</p>
          <center>
          <img src="static/images/manipulate_multilingual.png" alt="Manipulation Results Multilingual" class="teaser-image" width="150px">
          <img src="static/images/manipulate_monolingual.png" alt="Manipulation Results Monolingual" class="teaser-image" width="150px">
          <img src="static/images/manipulate_multimodal.png" alt="Manipulation Results Multimodal" class="teaser-image" width="90px">
        </center>
        <p>
          <center>
          <b><i>Fig 3: Causal Effect of Attention Imbalance.</i></b> Applying a fixed attention bias to increase attention over the underrepresented modality improves conflict detection performance dramatically‚Äîup to 43% absolute improvement in cross-lingual settings and 18% in cross-modal settings.
          </center>
        </p>

      <div style="background-color: rgba(173, 216, 230, 0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
        <p style="font-size: 20px; font-weight: bold;">‚úÖ Validation on Real Benchmarks</p>
        <p>We verify our findings on challenging vision-language benchmarks (BLINK, SAT). Attention manipulation achieves up to 2% absolute accuracy gains on datasets requiring substantive visual reasoning, confirming that reducing attention imbalance improves real-world performance.</p>
      </div>
      <div style="text-align: center; margin: 30px 0; padding: 20px; background-color: #f5f5f5; border-left: 5px solid #3273dc;">
        <p style="font-size: 24px; font-weight: bold; color: #3273dc; margin: 0;">
          üí° Cross-modal attention imbalance has a causal negative effect on cross-modal reasoning
        </p>
      </div>
    </div>
    </div>   
    </section>
    <section class="hero is-small">

      <div class="hero-body">
        
        <div class="container">
          <h2 class="title">The Solution: Instance-Level Modality Mixing</h2>
          <div class="content has-text-justified">
            <p><strong>Why doesn't simply training on diverse data help?</strong> Most state-of-the-art FMs today are trained on highly diverse corpora spanning multiple languages and modalities. However, this <strong>dataset-level modality mixing</strong> offers minimal gains in alleviating cross-modal attention imbalance.</p>
            
            <p>The problem: mixing datasets does not expose models to instances requiring cross-modal reasoning <strong>within the same context</strong>. Without instance-level mixing, pre-softmax attention scores for different modalities can be hugely different.</p>

            <h3 class="title">üí° Instance-Level Modality Mixing</h3>
            <p>We propose a simple and scalable method: <strong>explicitly combine multiple modalities within each training instance</strong>. For example:</p>

            <div style="background-color: rgba(240, 240, 240, 0.8);padding: 15px;margin: 15px 0;border-radius: 6px;border-left: 4px solid #3273dc;">
              <p style="font-weight: bold; margin-bottom: 10px;">Cross-Lingual Format:</p>
              <p style="margin: 5px 0;"><strong>Input:</strong> &lt;Chinese instruction&gt; &lt;English instruction&gt; Reply to both user instructions.</p>
              <p style="margin: 5px 0;"><strong>Output:</strong> &lt;Chinese response&gt; &lt;English response&gt;</p>
            </div>

            <div style="background-color: rgba(240, 240, 240, 0.8);padding: 15px;margin: 15px 0;border-radius: 6px;border-left: 4px solid #3273dc;">
              <p style="font-weight: bold; margin-bottom: 10px;">Cross-Modal Format:</p>
              <p style="margin: 5px 0;"><strong>Input:</strong> &lt;text instruction&gt; &lt;image&gt; &lt;image-related instruction&gt; Reply to both user instructions.</p>
              <p style="margin: 5px 0;"><strong>Output:</strong> &lt;text response&gt; &lt;image-related response&gt;</p>
            </div>

               <div style="background-color: rgba(225, 200, 245, 0.4);margin: 0px;padding-top: 10px;padding-bottom: 10px;width: 100%;padding-left: 20px;padding-right: 20px;border: 2px solid black;border-radius: 6px;;margin-bottom: 20px;">
                <p style="font-size: 20px; font-weight: bold;">üéØ Key Advantage</p>
                <p>Instance-level modality mixing is <strong>more scalable</strong> than directly finetuning on conflict detection tasks. It does NOT require explicit conflicts within instructions‚Äîwe simply repurpose existing datasets by concatenating examples from different modalities!</p>
              </div>

            <h3 class="title">üìä Results: Attention Imbalance Reduction</h3>
            <center>
            <img src="static/images/multilingual_finetune_attention.png" alt="Multilingual Finetuning Attention" class="teaser-image" width="200px">
            <img src="static/images/multimodal_finetune_attention.png" alt="Multimodal Finetuning Attention" class="teaser-image" width="200px">
            </center>
          
        <p>
          <center>
          <b><i>Fig 4: Attention Imbalance Reduction.</i></b> Instance-level modality mixing (dark blue) reduces attention imbalance between modalities by <strong>4√ó</strong> in cross-lingual settings and <strong>34%</strong> in cross-modal settings, far outperforming dataset-level mixing (gray).
          </center>
        </p>

        <h3 class="title">üìä Results: Conflict Detection Performance</h3>
            <center>
            <img src="static/images/multilingual_finetune_performance.png" alt="Multilingual Finetuning Performance" class="teaser-image" width="200px">
            <img src="static/images/multimodal_finetune_performance.png" alt="Multimodal Finetuning Performance" class="teaser-image" width="200px">
            </center>
          
        <p>
          <center>
          <b><i>Fig 5: Conflict Detection Improvement.</i></b> Instance-level modality mixing boosts conflict detection by <strong>37%</strong> in cross-lingual settings and <strong>2√ó</strong> in cross-modal settings compared to dataset-level mixing.
          </center>
        </p>
      </div>
    </div>
      </div>
      </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Validation on Downstream Tasks</h2>
          <div class="content has-text-justified">
            <p>
              Does instance-level modality mixing improve performance on real-world benchmarks? We test on several hard vision-language datasets that require non-trivial reasoning over each domain.
            </p>
            <h4 class="title">üî¨ Benchmarks</h4>
            <ul style="margin-left: 20px;">
              <li><strong>HardBLINK:</strong> Challenging vision-language reasoning</li>
              <li><strong>SAT (Static & Dynamic):</strong> Visual reasoning tasks</li>
              <li><strong>MMMU:</strong> Multimodal understanding benchmark</li>
            </ul>
            
            <center>
              <img src="static/images/instance_level_downstream.png" alt="Downstream Task Results" class="teaser-image" width="600px">
            </center>
            <p>
              <center>
              <b><i>Fig 6: Downstream Task Improvements.</i></b> Finetuning Qwen-2.5-VL on instance-level mixed data improves performance by 2.4% on HardBLINK, 2.4% on SAT (static), 4.4% on SAT (dynamic), and 1.2% on MMMU. These gains demonstrate that our method enhances overall reasoning capabilities, especially on datasets requiring substantive cross-modal reasoning.
              </center>
            </p>

            <div style="background-color: rgba(173, 216, 230, 0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
              <p style="font-size: 20px; font-weight: bold;">‚ú® Key Takeaway</p>
              <p>Instance-level modality mixing not only improves conflict detection but also enhances real-world performance on challenging multimodal benchmarks‚Äîwithout requiring additional data curation.</p>
            </div>
          </div>
    
            </section>
            
        </div>
      </section>
<!--End paper poster -->









<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wu2025mitigating,
        title={Mitigating Modal Imbalance in Multimodal Reasoning},
        author={Chen Henry Wu and Neil Kale and Aditi Raghunathan},
        institution={Carnegie Mellon University},
        year={2025}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
