<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of \emph{natural} sequences (those that resemble linguistically plausible text) become \emph{mechanistically entangled} with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of \seqtd that promotes isolation of memorization by design. We leverage a sequence identifier to activate a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that \seqtd facilitates clean isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement \seqtd at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at http://github.com/grghosal/MemSinks">
  <meta property="og:title" content="Memorization Sinks: Isolating Memorization during LLM Training"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://grghosal.github.io/memsinksweb/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/sink.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Memorization Sinks: Isolating Memorization during LLM Training </title>
  <link rel="icon" type="image/x-icon" href="static/images/sink.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Memorization Sinks: Isolating Memorization during LLM Training            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://grghosal.github.io/" target="_blank">Gaurav R. Ghosal</a>, 
                <span class="author-block">
                  <a href="https://pratyushmaini.github.io/" target="_blank">Pratyush Maini</a>, 
                  <span class="author-block">
                    <a href="https://www.cs.cmu.edu/~aditirag/" target="_blank">Aditi Raghunathan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Carnegie Mellon University<br>ICML 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <img src="static/images/MemSinksFig.png" alt="Teaser Image" style="width: 100%; max-width: 1000px; height: auto; display: block; margin: 10px auto;">
      </center>
      <h2 class="subtitle has-text-centered">
      Standard training of LLMs can lead to memorization being arbitrarily distributed across the model. As a result, removing it is costly and often degrades general capabilites. Our training technique, <strong>MemSinks</strong>, maintains a set of <it> sink neurons</it> to implement memorization. These memorization sinks are <it> removable by design</it>, enabling straightforward downstream unlearning.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <ul style="font-size: 1.2em; line-height: 1.6;">
            <li><strong>üö´ The Problem:</strong> Current LLMs memorize repeated text, creating privacy/copyright issues. Post-hoc removal methods fail because memorization gets entangled with general language abilities.</li>
            <li><strong>üí° The Solution:</strong> We introduce <strong>MemSinks</strong> - a training technique that isolates memorization into dedicated "sink neurons" that can be easily removed without affecting model performance.</li>
            <li><strong>‚úÖ The Results:</strong> First proof-of-concept showing simultaneous generalization and isolation is possible. Tested on billion-parameter models with real data - achieves both effective memorization removal and strong language capabilities.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Paper poster -->
<section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">The Limitations of Post-Hoc Memorization Removal</h2>
      <div class="content has-text-justified">
        <p>
          Present approaches for removing memorization primarily focus solely on post-hoc updates to the model weights (i.e. unlearning). However, these approaches rely on the assumption that memorization is sufficiently seperated from the model's general capabilities. We first study when this assumption is valid via controlled experiments.</p>

          <p>We study two settings:</p>
          <p>1. <strong>Natural Sequence Memorization:</strong> These are sequences that resemble linguistically plausible text (e.g. books, articles, etc). These often arise from frequently repeated data and present copyright and privacy concerns.</p>
          <p>2. <strong>Canaries:</strong> These are random or highly atypical sequences. They are often used in controlled studies of memorization. However, they can also arise in natual data (i.e. URLs, hashes, etc)</p>
          <p> We trained models exhibiting both kinds of memorization and evaluated two top-performing localization-based methods for removing them. For both methods, we examine the tradeoff between removing memorization (increasing loss on the memorized sequences) and preserving the model's general capabilities (maintaining loss on validation data). 
          </p>
          <p> <strong>Our findings (Figure 2) show that post-hoc methods particularly struggle to remove natural memorized sequences without compromising general capabilities</strong>. This is concerning as many real-world applications of unlearning (i.e. removing copyright violations) require removing sequences that may also be linguistically plausible.</p>

        </div>
        <center>
          <img src="static/images/IGInv.png" alt="Teaser Image" class="teaser-image" width  = "400px">
          <img src="static/images/HCInv.png" alt="Teaser Image" class="teaser-image" width  = "410px">
          </center>
          <p>
            <center>
              <b><i>Fig 2: Shortcomings of Post-Hoc Methods.</i></b> We show that state-of-the-art techniques for removing memorization induce a <it>tradeoff</it> between getting rid of memorization and preserving the model's general capabilities. This tradeoff is particularly amplified when memorized sequences consist of repeated, natural text. 
            </center>
          </p>
          <div style="background-color: rgba(225, 200, 245, 0.4);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
            <p style="font-size: 20px; font-weight: bold;">üß† Theoretical Intuition</p>
            <p>Why is memorized natural text challenging to remove post-hoc? In the paper, we theoretically analyze simplified linear neural network models and show that minimimum norm bias of gradient flow prefers solutions which reuse neurons for memorization and general language capabilities. </p>
          </div>
      </div>
    </div>
  </section>
<!--End paper poster -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">MemSinks: Isolating Memorization By Design</h2>
      <div class="content has-text-justified">
        <p >
          Given the shortcomings of removing memorization post-hoc, we next turn our attention to explicitly training models to enable the removal of memorization. We propose a new training paradigm, <strong>MemSinks</strong> to simultaneously achieve two goals:
          <div style="background-color: rgba(173, 216, 230, 0.5);margin: 0px;padding-top: 10px;padding-bottom: 10px;width: 100%;padding-left: 20px;padding-right: 20px;border: 2px solid black;border-radius: 6px;">
          <p style="font-size: 20px; font-weight: bold;">Desiderata </p>
          <p><strong>Isolate Memorization</strong>: Memorization should be stored in a known and removable set of neurons.</p>
          <p><strong>Preserve General Capabilities</strong>: The model should learn general capabilities from all data.</p>

          
          </div>

        </p>
        <p>We achieve this by proposing a training method (described next) which isolates memorization to special ``sink‚Äù neurons that can be straightforwardly removed post-hoc.
          </p>
          <h3 class="title">Implementing MemSinks</h3>

          <h4 class="title">Annotate Pretraining Data</h4>
          <center>
          <img src="static/images/AnnotateDataWhite.png" alt="Teaser Image" class="imp-image">
        </center>
          <p>
            The first step in implementing MemSinks is to annotate the training data with identifiers that reflect how we want sequences to be localized. Intuitively, tokens given the same identifier should be localized to the same sink neurons.  In our paper, we annotate use <it> document identifiers</it> to localize memorization arising from repetitions of the same document. This reflecs our focus in this work of eliminating verbatim memorization of pretraining data.</p>
   
        <div style="background-color: rgba(225, 200, 245, 0.4);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
          <p style="font-size: 20px; font-weight: bold;">&#128187; Implementation Details</p>
          <p><strong>Loading Sequence Annotations:</strong> We store sequence identifiers for each token and interleave them into the token stream, enabling efficient data loading while maintaining sequence-level information even when chunks cross document boundaries.</p>
          <p><strong>Efficiently Implementing Selective Activation of Sinks:</strong> We implement selective activation using deterministic binary masks computed from sequence identifiers. Our tensorized seeded random number generator efficiently computes activation masks on-the-fly, avoiding the need to pre-compute and store masks for every sequence.</p>
        </div>
        <div style="opacity: 1.0;background-color: rgba(173, 216, 230, 0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
          <p style="font-size: 20px; font-weight: bold;">&#9193; Future Work</p>
          <p><strong>Alternative Annotation Approaches:</strong> An interesting future direction is to explore alternative ways to annotate data. Leveraging information such as the document source, topic, or semantic clusters could potentially enable localization (and unlearning)
          at coarser levels of granularity. </p>
        </div>
      
        <h4 class="title">Designate MemSinks</h4>
          <center>
          <img src="static/images/DesignateMemorizationSinks.png" alt="Teaser Image" class="imp-image">
        </center>
        <p> Next, we need to designate the neurons that will store memorization (i.e. the sink neurons). In our work, we split the hidden MLP neurons at each transformer layer into two groups:<strong> sink neurons</strong> and <strong>general neurons</strong>. The ratio of sink neurons to general neurons is an important hyperparameter to tune (see paper for its effect). </p>
        <h4 class="title">Selectively Activate Sinks during Training</h4>
        <center>
          <img src="static/images/SelectiveActivation.png" alt="Teaser Image" class="imp-image">
        </center>
        <p> During training, the general neurons are always active (as in standard training). However, <strong> only a subset of sink neurons </strong> are activated on any given training update. The subset of sink neurons activated is determined using the sequence identifier annotations. This ensures that repeated data updates a consistent set of sink neurons throughout training.</p>
        <div style="background-color: rgba(225, 200, 245, 0.4);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
          <p style="font-size: 20px; font-weight: bold;">&#128187; Implementation Details</p>
          <p><strong>Loading Sequence Annotations:</strong> We store sequence identifiers for each token and interleave them into the token stream, enabling efficient data loading while maintaining sequence-level information even when chunks cross document boundaries. </p>
          <p><strong>Efficiently Implementing Selective Activation of Sinks:</strong> We implement selective activation using deterministic binary masks computed from sequence identifiers. Our tensorized seeded random number generator efficiently computes activation masks on-the-fly, avoiding the need to pre-compute and store masks for every sequence. </p>
        </div>      
      <h4 class="title">Throw Away Sinks</h4>
      <p> Given a model trained with MemSinks, we can remove memorized sequences by simply dropping out the sink neurons. This is particularly straightforward relative to post-hoc methods which require further finetuning or localization!</p>
      <div style="background-color: rgba(173, 216, 230, 0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 2px solid black;border-radius: 6px;">
        <p style="font-size: 20px; font-weight: bold;">&#9193; Future Work</p>
        <p><strong>Targeted Unlearning</strong> In this work, we focused primarily on removing memorization entirely from the model. As such, we primarily test the case of removing all sink neurons. However, an important direction for future work is to enable targeted removal of specific memorized sequences, while preserving others.</p>
      </div>
    </div>
    </div>   
    </section>
    <section class="hero is-small">

      <div class="hero-body">
        
        <div class="container">
          <h3 class="title">Small Scale Validation of MemSinks</h3>
          <div class="content has-text-justified">
            <p style="margin-bottom: 15px;">We first validated MemSinks on a small-scale setting based off of the TinyStories dataset, with a subset of 1000 stories repeated 1000 times.
               We compared standard trained models with MemSinks (where sinks were dropped out). Our in Figure 2 results provide compelling that MemSinks satisfies our two desiderata:
              </p>
               <div style="background-color: rgba(225, 200, 245, 0.4);margin: 0px;padding-top: 10px;padding-bottom: 10px;width: 100%;padding-left: 20px;padding-right: 20px;border: 2px solid black;border-radius: 6px;;margin-bottom: 20px;">
                <p style="font-size: 20px; font-weight: bold;">&#127942; Desiderata </p>
                <p> ‚úÖ  <strong>Isolate Memorization:</strong> In the middle panel of Figure 2, we see that the MemSinks model has significantly higher loss on the repeated stories than standard training.</p>
                <p> ‚úÖ <strong>Preserve General Capabilities:</strong> We see that MemSinks achieves comparable validation loss as standard training in the left panel of Figure 2. Moreover, the right panel of Figure 2 shows that MemSinks achieves a better tradeoff between removing memorization and preserving general capabilities than post-hoc methods.</p>
              </div>
            <center>
            <img src="static/images/MemSinksVal.png" alt="Teaser Image" class="teaser-image" width  = "400px">
            <img src="static/images/MemSinksMem.png" alt="Teaser Image" class="teaser-image" width  = "400px">
            <img src="static/images/SeqTiedTradeoffInv.png" alt="Teaser Image" class="teaser-image" width  = "410px">
            </center>
          
        <p>
          <center>
          <b><i>Fig 2: Performance of MemSinks.</i></b> (Left) MemSinks achieves comparable validation loss as standard training. (Center) MemSinks memorizes significantly less than standard training. (Right) MemSinks achieves a better tradeoff between removing memorization and preserving general capabilities than post-hoc methods.
          </center>
        </p>
      </div>
    </div>
      </div>
      </section>

        
        
    </div>
  </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Scaling MemSinks to 1B Parameter Model</h2>
          <div class="content has-text-justified">
            <p>
              Based on our preliminary evaluation, we next test MemSinks on a larger scale. We train on a mixture of SlimPajama and a small subset of Wikipedia documents. Our goal is to attain good performance on the Wikipedia dataset (via upsampling), while avoiding verbatim memorization of the repeated Wikipedia documents. In Figure 3, we see that MemSinks is capable of leveraging the repeated Wikipedia dataset for generalization (attaining improved performance on the Wikipedia dataset), while also avoiding verbatim memorization of the repeated Wikipedia documents (as occurs in standard training).
            </p>
              </p>
              <center>
                <img src="static/images/MemSinks17B.png" alt="Teaser Image" class="teaser-image" width  = "400px">
                <img src="static/images/MemSinks360.png" alt="Teaser Image" class="teaser-image" width  = "400px">
                </center>
    
    
    
            </div>
            <p>
              <center>
              <b><i>Fig 3: Scaling MemSinks to 1B Parameter Model.</i></b> Training loss curves for 1.7B (left) and 360M (right) parameter models trained on a mixture of Wikipedia and SlimPajama data.
              </center>
            </p>
          </div>
    
            </section>
            
        </div>
      </section>
<!--End paper poster -->









<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
        ghosal2025memorization,
        title={Memorization Sinks: Isolating Memorization during {LLM} Training},
        author={Gaurav Rohit Ghosal and Pratyush Maini and Aditi Raghunathan},
        booktitle={Forty-second International Conference on Machine Learning},
        year={2025},
        url={https://openreview.net/forum?id=sRJrMPu5Uu}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
