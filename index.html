<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of \emph{natural} sequences (those that resemble linguistically plausible text) become \emph{mechanistically entangled} with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of \seqtd that promotes isolation of memorization by design. We leverage a sequence identifier to activate a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that \seqtd facilitates clean isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement \seqtd at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at http://github.com/grghosal/MemSinks">
  <meta property="og:title" content="Memorization Sinks: Isolating Memorization during LLM Training"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/sink.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Memorization Sinks: Isolating Memorization during LLM Training </title>
  <link rel="icon" type="image/x-icon" href="static/images/sink.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Memorization Sinks: Isolating Memorization during LLM Training            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Gaurav R. Ghosal</a>, 
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Pratyush Maini</a>, 
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Aditi Raghunathan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Carnegie Mellon University<br>ICML 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/MemSinksFig.png" alt="Teaser Image" class="teaser-image">
      <h2 class="subtitle has-text-centered">
      Standard training of LLMs can lead to memorization being arbitrarily distributed across the model. As a result, removing it is costly and often degrades general capabilites. Our training technique, <strong>MemSinks</strong>, maintains a set of <it> sink neurons</it> to implement memorization. These memorization sinks are <it> removable by design</it>, enabling straightforward downstream unlearning.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of \emph{natural} sequences (those that resemble linguistically plausible text) become \emph{mechanistically entangled} with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of \seqtd that promotes isolation of memorization by design. We leverage a sequence identifier to activate a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that \seqtd facilitates clean isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement \seqtd at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at <a></a>http://github.com/grghosal/MemSinks</p>          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Paper poster -->
<section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">The Limitations of Post-Hoc Memorization Removal</h2>
      <div class="content has-text-justified">
        <p>
          Existing work on removing memorization hinges on the assumption that we can clearly delineate memorization from other LLM capabilities. Through a controlled study, we show that memorization is not always cleanly seperated from general language capabilities. This is particularly the case for memorized sequences resembling natural text (i.e. books, articles, etc). On the other hand, highly atypical memorized strings (i.e. random token sequences) can be naturally seperated. </p>

        </div>
        <center>
          <img src="static/images/IGInv.png" alt="Teaser Image" class="teaser-image" width  = "400px">
          <img src="static/images/HCInv.png" alt="Teaser Image" class="teaser-image" width  = "410px">
          </center>
          <p>
            <center>
              <b><i>Fig 2: Shortcomings of Post-Hoc Methods.</i></b> We show that state-of-the-art techniques for removing memorization induce a <it>tradeoff</it> between getting rid of memorization and preserving the model's general capabilities. This tradeoff is particularly amplified when memorized sequences consist of repeated, natural text. 
            </center>
          </p>
          <div style="background-color: rgba(255, 193, 8, 0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 4px solid black;border-radius: 15px;">
            <p style="font-size: 20px; font-weight: bold;">&#128161; Theoretical Intuition</p>
            <p>Why is memorized natural text challenging to remove post-hoc? In the paper, we theoretically analyze simplified linear neural network models and show that minimimum norm bias of gradient flow prefers solutions which reuse neurons for memorization and general language capabilites. </p>
          </div>
      </div>
    </div>
  </section>
<!--End paper poster -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">MemSinks: Isolating Memorization By Design</h2>
      <div class="content has-text-justified">
        <p >
          Given the shortcomings of removing memorization post-hoc, we next turn our attention to explicitly training models to enable the removal of memorization. We propose a new training paradigm, <strong>MemSinks</strong> to simultaneously achieve two goals:
          <div style="background-color: rgb(167, 224, 243, 0.5);margin: 0px;padding-top: 10px;padding-bottom: 10px;width: 100%;padding-left: 20px;padding-right: 20px;border: 4px solid black;border-radius: 15px;">
          <p style="font-size: 20px; font-weight: bold;">🙏 Desiredata </p>
          <p> &#9989;  <strong>Isolate Memorization</strong>: Memorization should be stored in a known and removable set of neurons.</p>
          <p>&#9989; <strong>Preserve Cross-Sequence Learning</strong>: The model should learn general capabilities from all data.</p>

          
          </div>

        </p>
        <p>We achieve this by proposing a training method (described next) which isolates memorization to special ``sink” neurons that can be straightforwardly removed post-hoc.
          </p>
          <h3 class="title">Implementing MemSinks</h3>

          <h4 class="title">Annotate Pretraining Data</h4>
          <center>
          <img src="static/images/AnnotateDataWhite.png" alt="Teaser Image" class="teaser-image" width  = 800px">
        </center>
          <p>
            The first step in implementing MemSinks is to annotate the training data with identifiers that reflect how we want sequences to be localized. Intuitively, tokens given the same identifier should be localized to the same sink neurons.  In our paper, we annotate use <it> document identifiers</it> to localize memorization arising from repetitions of the same document. This reflecs our focus in this work of eliminating verbatim memorization of pretraining data.</p>
   
        <div style="background-color: rgb(252, 165, 178,0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 4px solid black;border-radius: 15px;">
          <p style="font-size: 20px; font-weight: bold;">&#128187; Implementation Details</p>
          <p><strong>Generating Annotations</strong>There is considerable flexibility in how sequence ID annotations are provided, as long as similar sequences are assigned the same ID. In our work, we generate IDs during the tokenization process by computing the hash of the tokens in a document. This has minimal overhead over the standard tokenization process. </strong></p>
        </div>
        <div style="opacity: 1.0;background-color: rgba(191, 246, 191, 0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 4px solid black;border-radius: 15px;">
          <p style="font-size: 20px; font-weight: bold;">&#9193; Future Work</p>
          <p><strong>Alternative Annotation Approaches</strong> An interesting future direction is to explore alternative ways to annotate data. Leveraging information such as the document source, topic, or semantic clusters could potentially enable localization (and unlearning)
          at coarser levels of granularity. </p>
        </div>
      
        <h4 class="title">Designate MemSinks</h4>
          <center>
          <img src="static/images/DesignateMemorizationSinks.png" alt="Teaser Image" class="teaser-image" width  = "800px">
        </center>
        <p> Next, we need to designate the neurons that will store memorization (i.e. the sink neurons). In our work, we split the hidden MLP neurons at each transformer layer into two groups:<strong> sink neurons</strong> and <strong>general neurons</strong>. The ratio of sink neurons to general neurons is an important hyperparameter to tune (see paper for its effect). </p>
        <h4 class="title">Selectively Activate Sinks during Training</h4>
        <center>
          <img src="static/images/SelectiveActivation.png" alt="Teaser Image" class="teaser-image" width  = "600px">
        </center>
        <p> During training, the general neurons are always active (as in standard training). However, <strong> only a subset of sink neurons </strong> are activated on any given training update. The subset of sink neurons activated is determined using the sequence identifier annotations. This ensures that repeated data updates a consistent set of sink neurons throughout training.</p>
        <div style="background-color: rgb(252, 165, 178,0.5);padding-left: 20px;padding-right: 20px;margin: 10px;padding-top: 10px;padding-bottom: 10px;width: 100%;border: 4px solid black;border-radius: 15px;">
          <p style="font-size: 20px; font-weight: bold;">&#128187; Implementation Details</p>
          <p><strong>Loading Sequence Annotations</strong> Large scale pretraining implementations typically treat the data as a single stream of tokens, from which fixed-size chunks are sampled (potentially crossing document boundaries). As a result, accessing additional sequence level informaton can be challenging. We store a sequence identifier for <it> each token</it> in the dataset and interleave sequence IDs into the token stream. This enables us to use existing streaming dataloaders to efficiently sample data, while sequence IDs can be easily extracted using simple indexing operations. Moreover, maintaining sequence ID for each token allows easy handling of data that crosses document boundaries. </p>
          <p><strong>Efficiently Implementing Selective Activation of Sinks</strong> Selective activation can be implemented by simply multiplying the sink neuron activations by a binary mask (as in dropout). However, unlike stochastic dropout, the binary mask must be a deterministic function of the sequence identifier (while still balancing the number of documents for which a sink neuron is active). Moreover, as we maintain a sequence ID for each token, we must compute a seperate activation mask for each token in a batch. We developed a simple tensorized seeded random number generator to efficiently compute sink neuron activation masks during training. Our on-the-fly implementation prevents the need for pre-computing and storing masks corresponding for every pretraining sequence. </p>
        </div>
      </div>
    </div>


          

   
    </section>
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">



          <center>
            <img src="static/images/MemSinksVal.png" alt="Teaser Image" class="teaser-image" width  = "400px">
            <img src="static/images/MemSinksMem.png" alt="Teaser Image" class="teaser-image" width  = "400px">
            <img src="static/images/SeqTiedTradeoffInv.png" alt="Teaser Image" class="teaser-image" width  = "410px">
            </center>



     
        <p>
          <center>
          <b><i>Fig 2: Performance of MemSinks.</i></b> (Left) MemSinks achieves comparable validation loss as standard training. (Center) MemSinks memorizes significantly less than standard training. (Right) MemSinks achieves a better tradeoff between removing memorization and preserving general capabilities than post-hoc methods.
          </center>
        </p>
      </div>
      </div>
      </section>

        
        
    </div>
  </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Scaling MemSinks to 1B Parameter Model</h2>
          <div class="content has-text-justified">
            <p>
            Based on our preliminary results, we deployed MemSinks when training 360M and 1.7 B parameter models on a mixture of Wikipedia and SlimPajama data. We consider a setting where we wish to upsample the relatively rare Wikipedia data (to improve validation loss on Wikipedia documents). Upsampling the training Wikipedia data leads to a model that memorizes less than standard training, but is still able to generalize to Wikipedia documents.  Upsampling the training Wikipedia documents results in memorization, as seen in the gap betwen validation and training loss in the training curves below. We see that MemSinks enables us to mitigate this memorization while still improving Wikipedia validation loss using the repeated documents (outperforming deduplicated standard training).          
            </p>
              </p>
              <center>
                <img src="static/images/MemSinks17B.png" alt="Teaser Image" class="teaser-image" width  = "400px">
                <img src="static/images/MemSinks360.png" alt="Teaser Image" class="teaser-image" width  = "400px">
                </center>
    
    
    
            </div>
            <p>
              <center>
              <b><i>Fig 3: Scaling MemSinks to 1B Parameter Model.</i></b> Training loss curves for 1.7B (left) and 360M (right) parameter models trained on a mixture of Wikipedia and SlimPajama data.
              </center>
            </p>
          </div>
    
            </section>
            
        </div>
      </section>
<!--End paper poster -->









<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
        ghosal2025memorization,
        title={Memorization Sinks: Isolating Memorization during {LLM} Training},
        author={Gaurav Rohit Ghosal and Pratyush Maini and Aditi Raghunathan},
        booktitle={Forty-second International Conference on Machine Learning},
        year={2025},
        url={https://openreview.net/forum?id=sRJrMPu5Uu}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
